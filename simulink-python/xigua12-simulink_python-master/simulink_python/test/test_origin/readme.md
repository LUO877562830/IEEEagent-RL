### Step1

 1. 发现除了第一次是传输10次state以外，其余时刻都是传输4个state的信息
 
 2. 将其取平均之后发现采取相同的action会得到不同趋势的state，环境不稳定,准备先跑通这个之后再尝试CartPole的环境
 
 3. 使用dqn_test.py测试脚本的log如下:

    >bind server port success
    Wait 20 seconds for a response from client to server 50000
    Server connnection success ! Address :('127.0.0.1', 48334) , port :50000
    The receiver port binding success
    receiving connect success！Address :('127.0.0.1', 40358) ,port :50001
    1
    current state T1: 16.0 ,Tmix: 23.2 ,Treturn: 16.0
    [16.0, 23.2, 16.0]
    1
    ([15.0, 22.400000000000002, 16.88], 0.0, False, 'normal')  0
    10
    ([14.343, 18.060000000000002, 16.586000000000002], -1, False, 'normal')  1
    4
    ([14.245000000000001, 17.000000000000004, 15.962499999999999], -1, False, 'normal')  2
    4
    ([14.21, 20.2, 16.487499999999997], -1, False, 'normal')  0
    4
    ([14.200000000000001, 19.400000000000002, 16.7325], -1, False, 'normal')  1
    4
    ([14.192499999999999, 20.2, 16.5375], -1, False, 'normal')  2

### Step2

 1. 使用CartPole的环境需要一段时间的适配，所以先使用origin的环境进行测试，测试结果如下2,3,4,5点
 
 2. 减少step size可以提升结果的准确度，总的模拟的时间也会变长。所以调整step size不可以调整单次通信模拟的次数。调整单次通信模拟次数也就是调整一次step的时间。所以打开并调整pacing option

 3. 阻塞式通信方式(勾选Enable blocking mode)。simulink接收了python传输的action之后才能进行渲染,python传输action之后等待0.1s,再接收simulink传输的state。

 4. TCP/IP Receive有两个输出，一个data，一个state。state:(0,1)表示有没有接收到

 5. block sample time为当前模块的模拟step只要在max step size 和min step size之间就好了

### Questions

 1. 奖励值的图波动较大。原来奖励值不稳定很正常，有多个评价指标，某个指标超了来个负号，再乘上计算量，就会造成奖励值的波动。而且我们画图的时候用的是episode的总奖励值，所以波动会更大。而且在gym的环境中，一直操作有误的话它会done掉的，所以就会造成那个epoch的数据特别差。不过这都没关系，因为这种数据放到记忆库中也是可以学习的，对于学习来说，错误比正确更重要，从错误中学习(机器和人都一样)。
**从正确中学习？或者说从正确中创造错误进行学习？**

 2. 梯度消失也和奖励值的设置有很大的关系。简单来说，有奖励值的差值就会有梯度的存在。假设：某个评价指标变化两个单位的量奖励值才会变化一个单位。比如评价指标从18.33到18.35奖励值才从1.81变为1.82,这其中是两倍的关系，而经过了loss函数的计算，这个奖励值会更大或者更小,**loss函数如何对reward梯度进行影响?**。假设奖励值的设置只和这个评价指标有关。这时就会出现梯度的消失。会出现的问题为:不容易收敛，也就是说神经网络不容易很快学到。如果问题无法慢性解决的话(也就是靠着这些梯度前进的话)。环境会在当前的状态往回走，也就是说它会倒退，为什么呢？在没有梯度的情况下，为了更好的学习获取更多的reward，我要自己创造梯度，而我现在所处的环境是没有梯度的，那我就回去走走吧，借着前几个state的梯度冲一冲，达到下一个reward更高的state。这又会造成另一个问题，我的state往回走多少呢，如果走的太多，可能会导致一直处于这个阶段的某几个状态，也就是所谓的**local optim**。将local optim理解为一个盆地再好不过了。为了冲出这个盆地，我之前的速度不够，所以我退了下来，来到盆地的另一边，借助另一边的坡度，给予我速度。下次能不能冲过就看我有没有足够的速度了。但是这样的盆地并非只有一个，而且在高维空间中，盆地所在的维度也不一样。所以当你的reward在到达某处之后在不断下降，一点都不要方，你的神经网络可能在计算要冲过这个盆地，所需要的速度也就是梯度是多少，就是说它会回退多少。但是，他可能并不知道，这样会使问题变得更加复杂。因为，由于数据不同，每次更新之后神经网络的结构也不一样，所以他可能根本找不到原来的那个盆地了。**为什么神经网络的梯度会选择回退呢？**
当然，数据多就没事，数据量足够大的情况下(比如DQN的batch sample)就可以比较好的解决这个问题，只要数据是没问题的。数据够多，梯度自然存在。

 3. 这样的话之前的很好解释训练好的模型，当真正用时效果却很差。在最后很多次epoch的训练过程中，使用的是操作很好的经验值，所以神经网络储存的大部分映射信息被这些操作很好的经验值占据。**能不能将训练好的那部分神经网络固定住呢？用另一个神经网络进行控制？**